{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337e0db7",
   "metadata": {},
   "source": [
    "# LSTM Model Training for RUL Prediction (PyTorch)\n",
    "\n",
    "**Objective:** Build and train an LSTM neural network to predict Remaining Useful Life (RUL)\n",
    "\n",
    "**Steps:**\n",
    "1. Load preprocessed data\n",
    "2. Build LSTM model architecture with PyTorch\n",
    "3. Train model with early stopping ( In Progress )\n",
    "4. Evaluate performance (RMSE, MAE) ( In Progress )\n",
    "5. Visualize predictions and training history ( In Progress )\n",
    "6. Save trained model and extract latent features for XGBoost ( In Progress )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8bffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Import required libraries for deep learning and visualization\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107b1007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Load preprocessed data and metadata\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "X_train = np.load('processed_data/X_train.npy')\n",
    "y_train = np.load('processed_data/y_train.npy')\n",
    "X_val = np.load('processed_data/X_val.npy')\n",
    "y_val = np.load('processed_data/y_val.npy')\n",
    "X_test = np.load('processed_data/X_test.npy')\n",
    "y_test = np.load('processed_data/y_test.npy')\n",
    "\n",
    "with open('processed_data/metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nMetadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1).to(device)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"\\nData loaders created with batch size: 128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Build LSTM model architecture\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim1=64, hidden_dim2=32, fc_dim=32, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(n_features, hidden_dim1, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim2, fc_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2 = nn.Linear(fc_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM 1\n",
    "        lstm1_out, _ = self.lstm1(x)\n",
    "        lstm1_out = self.dropout1(lstm1_out)\n",
    "        \n",
    "        # LSTM 2\n",
    "        lstm2_out, _ = self.lstm2(lstm1_out)\n",
    "        lstm2_out = self.dropout2(lstm2_out)\n",
    "        \n",
    "        # Take last timestep output\n",
    "        lstm2_last = lstm2_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        fc1_out = self.fc1(lstm2_last)\n",
    "        fc1_out = self.relu(fc1_out)\n",
    "        fc1_out = self.dropout3(fc1_out)\n",
    "        \n",
    "        output = self.fc2(fc1_out)\n",
    "        \n",
    "        return output, lstm2_last  # Return both output and latent features\n",
    "    \n",
    "    def predict(self, x):\n",
    "        output, _ = self.forward(x)\n",
    "        return output\n",
    "\n",
    "# Build model\n",
    "sequence_length = metadata['sequence_length']\n",
    "n_features = metadata['n_features']\n",
    "\n",
    "model = LSTMModel(n_features=n_features).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=7, min_lr=1e-6, verbose=True\n",
    ")\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f0d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Define training and validation functions\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_mae = 0\n",
    "    \n",
    "    for batch_X, batch_y in loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs, _ = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_mae += torch.mean(torch.abs(outputs - batch_y)).item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_mae = total_mae / len(loader)\n",
    "    \n",
    "    return avg_loss, avg_mae\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_mae = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in loader:\n",
    "            outputs, _ = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_mae += torch.mean(torch.abs(outputs - batch_y)).item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_mae = total_mae / len(loader)\n",
    "    \n",
    "    return avg_loss, avg_mae\n",
    "\n",
    "print(\"Training functions defined\")\n",
    "print(\"  - Early stopping patience: 15 epochs\")\n",
    "print(\"  - Learning rate reduction patience: 7 epochs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
