{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337e0db7",
   "metadata": {},
   "source": [
    "# LSTM Model Training for RUL Prediction (PyTorch)\n",
    "\n",
    "**Objective:** Build and train an LSTM neural network to predict Remaining Useful Life (RUL)\n",
    "\n",
    "**Steps:**\n",
    "1. Load preprocessed data\n",
    "2. Build LSTM model architecture with PyTorch\n",
    "3. Train model with early stopping \n",
    "4. Evaluate performance (RMSE, MAE) \n",
    "5. Visualize predictions and training history \n",
    "6. Save trained model and extract latent features for XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8bffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Import required libraries for deep learning and visualization\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107b1007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Load preprocessed data and metadata\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "X_train = np.load('processed_data/X_train.npy')\n",
    "y_train = np.load('processed_data/y_train.npy')\n",
    "X_val = np.load('processed_data/X_val.npy')\n",
    "y_val = np.load('processed_data/y_val.npy')\n",
    "X_test = np.load('processed_data/X_test.npy')\n",
    "y_test = np.load('processed_data/y_test.npy')\n",
    "\n",
    "with open('processed_data/metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nMetadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1).to(device)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"\\nData loaders created with batch size: 128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Build LSTM model architecture\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim1=64, hidden_dim2=32, fc_dim=32, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(n_features, hidden_dim1, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(hidden_dim1, hidden_dim2, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_dim2, fc_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2 = nn.Linear(fc_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM 1\n",
    "        lstm1_out, _ = self.lstm1(x)\n",
    "        lstm1_out = self.dropout1(lstm1_out)\n",
    "        \n",
    "        # LSTM 2\n",
    "        lstm2_out, _ = self.lstm2(lstm1_out)\n",
    "        lstm2_out = self.dropout2(lstm2_out)\n",
    "        \n",
    "        # Take last timestep output\n",
    "        lstm2_last = lstm2_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        fc1_out = self.fc1(lstm2_last)\n",
    "        fc1_out = self.relu(fc1_out)\n",
    "        fc1_out = self.dropout3(fc1_out)\n",
    "        \n",
    "        output = self.fc2(fc1_out)\n",
    "        \n",
    "        return output, lstm2_last  # Return both output and latent features\n",
    "    \n",
    "    def predict(self, x):\n",
    "        output, _ = self.forward(x)\n",
    "        return output\n",
    "\n",
    "# Build model\n",
    "sequence_length = metadata['sequence_length']\n",
    "n_features = metadata['n_features']\n",
    "\n",
    "model = LSTMModel(n_features=n_features).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=7, min_lr=1e-6, verbose=True\n",
    ")\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f0d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Define training and validation functions\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_mae = 0\n",
    "    \n",
    "    for batch_X, batch_y in loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs, _ = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_mae += torch.mean(torch.abs(outputs - batch_y)).item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_mae = total_mae / len(loader)\n",
    "    \n",
    "    return avg_loss, avg_mae\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_mae = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in loader:\n",
    "            outputs, _ = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_mae += torch.mean(torch.abs(outputs - batch_y)).item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_mae = total_mae / len(loader)\n",
    "    \n",
    "    return avg_loss, avg_mae\n",
    "\n",
    "print(\"Training functions defined\")\n",
    "print(\"  - Early stopping patience: 15 epochs\")\n",
    "print(\"  - Learning rate reduction patience: 7 epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da94bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Train the model\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100\n",
    "patience = 15\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# History tracking\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_mae': [],\n",
    "    'val_mae': []\n",
    "}\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss, train_mae = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_mae = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_mae'].append(train_mae)\n",
    "    history['val_mae'].append(val_mae)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train MAE: {train_mae:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}\")\n",
    "    \n",
    "    # Early stopping and model checkpoint\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, 'models/lstm_best_model.pth')\n",
    "        print(f\"  --> Model saved (val_loss improved to {val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load('models/lstm_best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b449e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Visualize training history\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history['train_loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss (MSE)', fontsize=12)\n",
    "axes[0].set_title('Model Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE plot\n",
    "axes[1].plot(history['train_mae'], label='Training MAE', linewidth=2)\n",
    "axes[1].plot(history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('MAE', fontsize=12)\n",
    "axes[1].set_title('Mean Absolute Error Over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('models/training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training stopped at epoch: {len(history['train_loss'])}\")\n",
    "print(f\"Best validation loss: {min(history['val_loss']):.4f}\")\n",
    "print(f\"Best validation MAE: {min(history['val_mae']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c4d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Evaluate model on test set\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def predict_all(model, loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in loader:\n",
    "            outputs, _ = model(batch_X)\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "            actuals.append(batch_y.cpu().numpy())\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0).flatten()\n",
    "    actuals = np.concatenate(actuals, axis=0).flatten()\n",
    "    \n",
    "    return predictions, actuals\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train, _ = predict_all(model, train_loader, device)\n",
    "y_pred_val, _ = predict_all(model, val_loader, device)\n",
    "y_pred_test, _ = predict_all(model, test_loader, device)\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred, dataset_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Metrics:\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE:  {mae:.4f}\")\n",
    "    print(f\"  R²:   {r2:.4f}\")\n",
    "    \n",
    "    return {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "train_metrics = calculate_metrics(y_train, y_pred_train, \"Training\")\n",
    "val_metrics = calculate_metrics(y_val, y_pred_val, \"Validation\")\n",
    "test_metrics = calculate_metrics(y_test, y_pred_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4187140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Visualize predictions vs actual RUL\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Test set: Scatter plot\n",
    "axes[0, 0].scatter(y_test, y_pred_test, alpha=0.5, s=20)\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 0].set_xlabel('Actual RUL', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Predicted RUL', fontsize=12)\n",
    "axes[0, 0].set_title(f'Test Set: Predictions vs Actual\\nRMSE: {test_metrics[\"rmse\"]:.2f}', \n",
    "                     fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test set: Residual plot\n",
    "residuals = y_test - y_pred_test\n",
    "axes[0, 1].scatter(y_pred_test, residuals, alpha=0.5, s=20)\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Predicted RUL', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Residuals', fontsize=12)\n",
    "axes[0, 1].set_title('Test Set: Residual Plot', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Sample predictions over time (first 500 samples)\n",
    "sample_size = min(500, len(y_test))\n",
    "axes[1, 0].plot(y_test[:sample_size], label='Actual RUL', linewidth=2, alpha=0.8)\n",
    "axes[1, 0].plot(y_pred_test[:sample_size], label='Predicted RUL', linewidth=2, alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Sample Index', fontsize=12)\n",
    "axes[1, 0].set_ylabel('RUL', fontsize=12)\n",
    "axes[1, 0].set_title('Test Set: Sample Predictions (First 500)', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution\n",
    "axes[1, 1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Prediction Error (Actual - Predicted)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 1].set_title('Test Set: Error Distribution', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('models/predictions_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f193abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Extract latent features from LSTM for XGBoost\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def extract_features(model, loader, device):\n",
    "    \"\"\"Extract latent features from LSTM layer\"\"\"\n",
    "    model.eval()\n",
    "    features = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, _ in loader:\n",
    "            _, latent = model(batch_X)\n",
    "            features.append(latent.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(features, axis=0)\n",
    "\n",
    "# Extract features\n",
    "train_features = extract_features(model, train_loader, device)\n",
    "val_features = extract_features(model, val_loader, device)\n",
    "test_features = extract_features(model, test_loader, device)\n",
    "\n",
    "print(f\"Extracted latent features:\")\n",
    "print(f\"  Training: {train_features.shape}\")\n",
    "print(f\"  Validation: {val_features.shape}\")\n",
    "print(f\"  Test: {test_features.shape}\")\n",
    "\n",
    "# Save latent features\n",
    "np.save('processed_data/train_features_lstm.npy', train_features)\n",
    "np.save('processed_data/val_features_lstm.npy', val_features)\n",
    "np.save('processed_data/test_features_lstm.npy', test_features)\n",
    "\n",
    "print(\"\\nLatent features saved to 'processed_data/' folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bbe2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Save model and results\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'history': history,\n",
    "    'metadata': metadata\n",
    "}, 'models/lstm_final_model.pth')\n",
    "print(\"Model saved to 'models/lstm_final_model.pth'\")\n",
    "\n",
    "# Save predictions\n",
    "np.save('models/y_pred_test.npy', y_pred_test)\n",
    "print(\"Test predictions saved to 'models/y_pred_test.npy'\")\n",
    "\n",
    "# Save metrics\n",
    "results = {\n",
    "    'train_metrics': train_metrics,\n",
    "    'val_metrics': val_metrics,\n",
    "    'test_metrics': test_metrics,\n",
    "    'training_history': history,\n",
    "    'model_architecture': {\n",
    "        'sequence_length': sequence_length,\n",
    "        'n_features': n_features,\n",
    "        'total_params': sum(p.numel() for p in model.parameters())\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('models/training_results.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "print(\"Training results saved to 'models/training_results.pkl'\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Complete\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  RMSE: {test_metrics['rmse']:.4f} cycles\")\n",
    "print(f\"  MAE:  {test_metrics['mae']:.4f} cycles\")\n",
    "print(f\"  R²:   {test_metrics['r2']:.4f}\")\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"  - models/lstm_best_model.pth\")\n",
    "print(f\"  - models/lstm_final_model.pth\")\n",
    "print(f\"  - models/training_results.pkl\")\n",
    "print(f\"  - processed_data/train_features_lstm.npy\")\n",
    "print(f\"  - processed_data/val_features_lstm.npy\")\n",
    "print(f\"  - processed_data/test_features_lstm.npy\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
